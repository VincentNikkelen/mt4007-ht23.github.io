# Lecture 5 (Processing Data)

### Resources

- [P4DA](https://wesmckinney.com/book/): chapter 7.1-7.2
- [R4DS](https://r4ds.had.co.nz/index.html): chapter 12.1-12.2, 12.5
- [Dealing with outliers (motivation)](https://www.analyticsvidhya.com/blog/2021/05/detecting-and-treating-outliers-treating-the-odd-one-out/)
- [Dealing with missing data](https://www.analyticsvidhya.com/blog/2021/10/handling-missing-value/)

*You can find examples and motivation in the resources.*

## Overview

In this lecture we will discuss how to clean datasets. In reality, the raw data
we are presented with is typically fraught with erros and how we deal with these
erros is the process of *cleaning* the data. The focus of this lecture will be
on handling missing data, handling outliers and removing duplicates. This
process should be included in EDA.

### Cleaning the data

So far, we haven't put any thought into the *cleanliness* of the data, we have
assumed that it is already in a prestine state. That is, we already assume that
it can be "trusted" and worked with out of the box. This is typically not the
case. When we talk about cleanliness, there are a few things that need to be
adressed. 

#### Missing Data

In the course so far, we have looked at datasets that contain missing data and we
have ignored them. Doing this without justification is wrong, when
remvoing or adding data, there needs to be clear justification about why the
operation has been performed. That requires some thought and analysis, maybe a
round of [EDA](/lectures/4). If you have a reasonable
guess about the value of the missing data then *imputing* the data is a fine. If
there is no information to be found, then discarding the entry is fine. Either
way, making sure that you document what you have done is important.

#### Outliers 

An outliers is an entry that differs from the other entries significantly. The
reason for the outlier can vary. Typically, it is a data entry error,
measurement error, sampling error, processing error and there are ofcourse
natural outliers. There are many statistical methods that are employed for
detecting such data points. We will employ [EDA](/lectures/4) to detect and
delete/transform/impute/accept. Once again, whatever you choose be transparent
and document what you have done. 

#### Duplicates

In some datasets, there might exist entries that are duplicate. Ignoring
duplicates may skew the distribution of the data and should therefore be removed
from the dataset. Make sure that the entries are true duplicates, it is possible
that they have the same value but are not duplicate. This depends on the
dataset. Use your best judgement.

